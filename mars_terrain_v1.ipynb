{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pushkar-hue/MarsSimNav/blob/main/mars_terrain_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijZDUdEejr_P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "url = \"https://data.nasa.gov/docs/legacy/ai4mars-dataset-merged-0.1.zip\"\n",
        "zip_path = \"ai4mars-dataset.zip\"\n",
        "extract_dir = \"ai4mars-dataset\"\n",
        "\n",
        "# Download with progress bar\n",
        "def download_file(url, dest_path):\n",
        "    response = requests.get(url, stream=True)\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "    with open(dest_path, 'wb') as f, tqdm(\n",
        "        desc=dest_path,\n",
        "        total=total_size,\n",
        "        unit='iB',\n",
        "        unit_scale=True,\n",
        "        unit_divisor=1024,\n",
        "    ) as bar:\n",
        "        for chunk in response.iter_content(chunk_size=1024):\n",
        "            size = f.write(chunk)\n",
        "            bar.update(size)\n",
        "\n",
        "# Download if not already done\n",
        "if not os.path.exists(zip_path):\n",
        "    print(\"Downloading dataset...\")\n",
        "    download_file(url, zip_path)\n",
        "else:\n",
        "    print(\"ZIP file already downloaded.\")\n",
        "\n",
        "# Unzip\n",
        "if not os.path.exists(extract_dir):\n",
        "    print(\"Extracting...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    print(\"Done.\")\n",
        "else:\n",
        "    print(\"Dataset already extracted.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPEjW9fLj2Xv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def walk_through(dir_path):\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcDoEt_dkSGb"
      },
      "outputs": [],
      "source": [
        "#walk_through(extract_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXbPWkOhnkvX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Input paths\n",
        "root_dir = \"ai4mars-dataset/ai4mars-dataset-merged-0.1/msl\"\n",
        "images_dir = os.path.join(root_dir, \"images\", \"edr\")\n",
        "labels_dir = os.path.join(root_dir, \"labels\", \"train\")\n",
        "\n",
        "# Output paths\n",
        "subset_dir = \"ai4mars-subset\"\n",
        "subset_images = os.path.join(subset_dir, \"images\")\n",
        "subset_labels = os.path.join(subset_dir, \"labels\")\n",
        "\n",
        "# Make output dirs\n",
        "os.makedirs(subset_images, exist_ok=True)\n",
        "os.makedirs(subset_labels, exist_ok=True)\n",
        "\n",
        "# Build base filename sets\n",
        "image_dict = {f.rsplit(\".\", 1)[0]: f for f in os.listdir(images_dir) if f.lower().endswith(\".jpg\")}\n",
        "label_dict = {f.rsplit(\".\", 1)[0]: f for f in os.listdir(labels_dir) if f.lower().endswith(\".png\")}\n",
        "\n",
        "# Intersection of base names\n",
        "common_basenames = sorted(set(image_dict.keys()) & set(label_dict.keys()))\n",
        "print(f\"âœ… Found {len(common_basenames)} matched image-label pairs.\")\n",
        "\n",
        "# Select up to 5000\n",
        "subset_size = min(5000, len(common_basenames))\n",
        "subset_basenames = random.sample(common_basenames, subset_size)\n",
        "\n",
        "# Copy matched files\n",
        "print(f\"Copying {subset_size} pairs to 'ai4mars-subset/'...\")\n",
        "for base in tqdm(subset_basenames):\n",
        "    shutil.copy(os.path.join(images_dir, image_dict[base]), os.path.join(subset_images, image_dict[base]))\n",
        "    shutil.copy(os.path.join(labels_dir, label_dict[base]), os.path.join(subset_labels, label_dict[base]))\n",
        "\n",
        "print(\"Subset creation complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64y_Q08Yntsi"
      },
      "outputs": [],
      "source": [
        "walk_through(subset_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtaTNoZPn6b5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Paths\n",
        "subset_images = \"ai4mars-subset/images\"\n",
        "subset_labels = \"ai4mars-subset/labels\"\n",
        "\n",
        "# Get all matching base filenames\n",
        "image_files = sorted([f for f in os.listdir(subset_images) if f.lower().endswith(\".jpg\")])\n",
        "label_files = sorted([f for f in os.listdir(subset_labels) if f.lower().endswith(\".png\")])\n",
        "base_names = sorted(set(f.rsplit('.', 1)[0] for f in image_files) &\n",
        "                    set(f.rsplit('.', 1)[0] for f in label_files))\n",
        "\n",
        "# Pick a few random pairs\n",
        "sample_bases = random.sample(base_names, 3)\n",
        "\n",
        "# Label class mapping (visual only)\n",
        "label_colors = {\n",
        "    0: [0, 0, 0],        # soil\n",
        "    1: [100, 100, 100],  # bedrock\n",
        "    2: [255, 255, 0],    # sand\n",
        "    3: [255, 0, 0],      # big rock\n",
        "    255: [255, 255, 255] # null (white)\n",
        "}\n",
        "\n",
        "def decode_label_mask(mask):\n",
        "    decoded = np.zeros((*mask.shape[:2], 3), dtype=np.uint8)\n",
        "    for val, color in label_colors.items():\n",
        "        decoded[(mask == val).all(axis=-1)] = color\n",
        "    return decoded\n",
        "\n",
        "# Display\n",
        "for base in sample_bases:\n",
        "    img_path = os.path.join(subset_images, base + \".JPG\")\n",
        "    lbl_path = os.path.join(subset_labels, base + \".png\")\n",
        "\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    label = cv2.imread(lbl_path)\n",
        "    label = decode_label_mask(label)\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(\"Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(label)\n",
        "    plt.title(\"Decoded Label\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    overlay = cv2.addWeighted(img, 0.7, label, 0.3, 0)\n",
        "    plt.imshow(overlay)\n",
        "    plt.title(\"Overlay\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNQkBX0roMhS"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chvqH1W1Cdpc"
      },
      "outputs": [],
      "source": [
        "RGB_CLASS_MAP = {\n",
        "    (0, 0, 0): 0,           # soil\n",
        "    (1, 1, 1): 1,           # bedrock\n",
        "    (2, 2, 2): 2,           # sand\n",
        "    (3, 3, 3): 3,           # big rock\n",
        "    (255, 255, 255): 255    # ignore\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bu7N5F3MCgfu"
      },
      "outputs": [],
      "source": [
        "def encode_mask(mask_img):\n",
        "    mask_np = np.array(mask_img)\n",
        "    h, w, _ = mask_np.shape\n",
        "    label = np.ones((h, w), dtype=np.uint8) * 255  # default = ignore\n",
        "    for rgb, idx in RGB_CLASS_MAP.items():\n",
        "        matches = np.all(mask_np == rgb, axis=-1)\n",
        "        label[matches] = idx\n",
        "    return label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INjaugAjCofp"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MarsSegmentationDataset(Dataset):\n",
        "    def __init__(self, image_dir, label_dir, transform=None, img_size=256):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.images = sorted([f for f in os.listdir(image_dir) if f.endswith(\".JPG\")])\n",
        "        self.transform = transform\n",
        "        self.img_size = img_size\n",
        "\n",
        "        self.image_transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      img_name = self.images[idx]\n",
        "      base_name = img_name.rsplit(\".\", 1)[0]\n",
        "      label_name = base_name + \".png\"\n",
        "\n",
        "      # Load and resize\n",
        "      img = Image.open(os.path.join(self.image_dir, img_name)).convert(\"RGB\")\n",
        "      label_img = Image.open(os.path.join(self.label_dir, label_name)).convert(\"RGB\")\n",
        "      label_img = label_img.resize((self.img_size, self.img_size), resample=Image.NEAREST)\n",
        "\n",
        "      # Encode label\n",
        "      label = encode_mask(label_img)\n",
        "\n",
        "      # Apply transforms\n",
        "      img = self.image_transform(img)\n",
        "      label = torch.from_numpy(label).long()\n",
        "\n",
        "      return img, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsnCJw1XCraN"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split, DataLoader\n",
        "# Full dataset (your 5k subset)\n",
        "full_dataset = MarsSegmentationDataset(\n",
        "    image_dir=\"ai4mars-subset/images\",\n",
        "    label_dir=\"ai4mars-subset/labels\",\n",
        "    img_size=256\n",
        ")\n",
        "\n",
        "# Split sizes\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size\n",
        "\n",
        "# Random split\n",
        "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "# Loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKgxrJV9DTiO"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRJUlQOyCtbw"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torch.nn as nn\n",
        "\n",
        "# Load model\n",
        "model = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
        "\n",
        "# Replace classifier with 4 output classes (soil, bedrock, sand, big rock)\n",
        "model.classifier = torchvision.models.segmentation.deeplabv3.DeepLabHead(2048, 4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__el0O_MDQnW"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Msn40CPUDcJi"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Loss: CrossEntropy with ignore_index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lp4yfl28DoeP"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, masks in tqdm(loader, desc=\"Training\"):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)['out']\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    return running_loss / len(loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kh5GSydDrqj"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for images, masks in loader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)['out']\n",
        "            loss = criterion(outputs, masks)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = outputs.argmax(1)\n",
        "            valid = masks != 255\n",
        "            correct += (preds[valid] == masks[valid]).sum().item()\n",
        "            total += valid.sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return total_loss / len(loader), accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anx9v4wZDtoW"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 3\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_acc = evaluate(model, test_loader, criterion)\n",
        "\n",
        "    print(f\"[{epoch+1}/{EPOCHS}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
        "    scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_NjGfzBDvqz"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"deeplabv3_mars.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbuc0POhHlY-"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"deeplabv3_mars.pth\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNk834vm6o143+0SGN/j334",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
